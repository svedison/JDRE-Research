{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491e82ac-728d-4e79-b096-66946806a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.8.0.dev20250319+cu128)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.8.29-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m259.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading regex-2025.8.29-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m201.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, scipy, safetensors, regex, joblib, hf-xet, faiss-cpu, scikit-learn, pandas, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.12.0 hf-xet-1.1.9 huggingface-hub-0.34.4 joblib-1.5.2 pandas-2.3.2 pytz-2025.2 regex-2025.8.29 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 threadpoolctl-3.6.0 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.0 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu sentence-transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2064a9-2279-4271-b8dc-870e3f51407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.1.2)\n",
      "Downloading biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.85\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bc5289-3ff6-4037-8f71-e7999f4d20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m248.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3b54f3-698a-4e16-8ed4-b6303e0714a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from time import sleep\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import random\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa05d233-de6c-477d-ab38-4ea30f266700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed for: biomedical research\n",
      "Found 9999 articles.\n",
      "Fetching batch: 1–100\n",
      "Fetching batch: 101–200\n",
      "Fetching batch: 201–300\n",
      "Fetching batch: 301–400\n",
      "Fetching batch: 401–500\n",
      "Fetching batch: 501–600\n",
      "Fetching batch: 601–700\n",
      "Fetching batch: 701–800\n",
      "Fetching batch: 801–900\n",
      "Fetching batch: 901–1000\n",
      "Fetching batch: 1001–1100\n",
      "Fetching batch: 1101–1200\n",
      "Fetching batch: 1201–1300\n",
      "Fetching batch: 1301–1400\n",
      "Fetching batch: 1401–1500\n",
      "Fetching batch: 1501–1600\n",
      "Fetching batch: 1601–1700\n",
      "Fetching batch: 1701–1800\n",
      "Fetching batch: 1801–1900\n",
      "Fetching batch: 1901–2000\n",
      "Fetching batch: 2001–2100\n",
      "Fetching batch: 2101–2200\n",
      "Fetching batch: 2201–2300\n",
      "Fetching batch: 2301–2400\n",
      "Fetching batch: 2401–2500\n",
      "Fetching batch: 2501–2600\n",
      "Fetching batch: 2601–2700\n",
      "Fetching batch: 2701–2800\n",
      "Fetching batch: 2801–2900\n",
      "Fetching batch: 2901–3000\n",
      "Fetching batch: 3001–3100\n",
      "Fetching batch: 3101–3200\n",
      "Fetching batch: 3201–3300\n",
      "Fetching batch: 3301–3400\n",
      "Fetching batch: 3401–3500\n",
      "Fetching batch: 3501–3600\n",
      "Fetching batch: 3601–3700\n",
      "Fetching batch: 3701–3800\n",
      "Fetching batch: 3801–3900\n",
      "Fetching batch: 3901–4000\n",
      "Fetching batch: 4001–4100\n",
      "Fetching batch: 4101–4200\n",
      "Fetching batch: 4201–4300\n",
      "Fetching batch: 4301–4400\n",
      "Fetching batch: 4401–4500\n",
      "Fetching batch: 4501–4600\n",
      "Fetching batch: 4601–4700\n",
      "Fetching batch: 4701–4800\n",
      "Fetching batch: 4801–4900\n",
      "Fetching batch: 4901–5000\n",
      "Fetching batch: 5001–5100\n",
      "Fetching batch: 5101–5200\n",
      "Fetching batch: 5201–5300\n",
      "Fetching batch: 5301–5400\n",
      "Fetching batch: 5401–5500\n",
      "Fetching batch: 5501–5600\n",
      "Fetching batch: 5601–5700\n",
      "Fetching batch: 5701–5800\n",
      "Fetching batch: 5801–5900\n",
      "Fetching batch: 5901–6000\n",
      "Fetching batch: 6001–6100\n",
      "Fetching batch: 6101–6200\n",
      "Fetching batch: 6201–6300\n",
      "Fetching batch: 6301–6400\n",
      "Fetching batch: 6401–6500\n",
      "Fetching batch: 6501–6600\n",
      "Fetching batch: 6601–6700\n",
      "Fetching batch: 6701–6800\n",
      "Fetching batch: 6801–6900\n",
      "Fetching batch: 6901–7000\n",
      "Fetching batch: 7001–7100\n",
      "Fetching batch: 7101–7200\n",
      "Fetching batch: 7201–7300\n",
      "Fetching batch: 7301–7400\n",
      "Fetching batch: 7401–7500\n",
      "Fetching batch: 7501–7600\n",
      "Fetching batch: 7601–7700\n",
      "Fetching batch: 7701–7800\n",
      "Fetching batch: 7801–7900\n",
      "Fetching batch: 7901–8000\n",
      "Fetching batch: 8001–8100\n",
      "Fetching batch: 8101–8200\n",
      "Fetching batch: 8201–8300\n",
      "Fetching batch: 8301–8400\n",
      "Fetching batch: 8401–8500\n",
      "Fetching batch: 8501–8600\n",
      "Fetching batch: 8601–8700\n",
      "Fetching batch: 8701–8800\n",
      "Fetching batch: 8801–8900\n",
      "Fetching batch: 8901–9000\n",
      "Fetching batch: 9001–9100\n",
      "Fetching batch: 9101–9200\n",
      "Fetching batch: 9201–9300\n",
      "Fetching batch: 9301–9400\n",
      "Fetching batch: 9401–9500\n",
      "Fetching batch: 9501–9600\n",
      "Fetching batch: 9601–9700\n",
      "Fetching batch: 9701–9800\n",
      "Fetching batch: 9801–9900\n",
      "Fetching batch: 9901–9999\n",
      "\n",
      "Done. Saved clean abstracts to: pubmed_abstracts.txt\n"
     ]
    }
   ],
   "source": [
    "Entrez.email = \"edisonzjy@gmail.com\"\n",
    "search_term = \"biomedical research\"\n",
    "max_articles = 10000\n",
    "batch_size = 100\n",
    "\n",
    "output_file = \"pubmed_abstracts.txt\"\n",
    "\n",
    "def fetch_pubmed_abstracts():\n",
    "    print(f\"Searching PubMed for: {search_term}\")\n",
    "    search_handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_articles)\n",
    "    record = Entrez.read(search_handle)\n",
    "    search_handle.close()\n",
    "\n",
    "    pmids = record[\"IdList\"]\n",
    "    print(f\"Found {len(pmids)} articles.\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for start in range(0, len(pmids), batch_size):\n",
    "            end = min(start + batch_size, len(pmids))\n",
    "            batch_pmids = pmids[start:end]\n",
    "            print(f\"Fetching batch: {start + 1}–{end}\")\n",
    "            try:\n",
    "                handle = Entrez.efetch(\n",
    "                    db=\"pubmed\",\n",
    "                    id=\",\".join(batch_pmids),\n",
    "                    rettype=\"abstract\",\n",
    "                    retmode=\"text\"\n",
    "                )\n",
    "                data = handle.read()\n",
    "                handle.close()\n",
    "\n",
    "                articles = []\n",
    "                raw_entries = data.split(\"\\n\\n\")\n",
    "                for entry in raw_entries:\n",
    "                    match = re.search(r\"(?:\\n\\n)?(?:[A-Z].+?\\.){2,}\", entry, re.DOTALL)\n",
    "                    if match:\n",
    "                        cleaned = match.group().strip().replace(\"\\n\", \" \")\n",
    "                        articles.append(cleaned)\n",
    "                for article in articles:\n",
    "                    f.write(article + \"\\n\")\n",
    "\n",
    "                sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching batch {start + 1}–{end}: {e}\")\n",
    "                sleep(2)\n",
    "\n",
    "    print(f\"\\nDone. Saved clean abstracts to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_pubmed_abstracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00437c-4f65-4ee8-9318-847ba128c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c23fe-c62e-41e7-94c1-4e27e74a2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare biobert, kebiolm, sapbert, and biolinkbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a631f1c-ae3f-48bd-bc13-d84ed292d9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loading documents...\n",
      "Total documents: 3332\n",
      "\n",
      "🔍 Loading model: BioBERT\n",
      "🔬 Embedding documents with BioBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60a378ffb4d43218a6a26fa25118a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building FAISS index for BioBERT...\n",
      "\n",
      "🔍 Loading model: SapBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cambridgeltl/SapBERT-from-PubMedBERT-fulltext. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a1712af1fe460db692c62e2bd90d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/462 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591d4c21d0b1487e84e5730f7d9d66bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417ef33f34344854964a7499623cada8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdad601a5805463587e1b0d07619e78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e458f0d1fb44b3194aaf008ab737c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Embedding documents with SapBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611c0cadcfa14cd6a9ab799181e51ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building FAISS index for SapBERT...\n",
      "\n",
      "🔍 Loading model: MiniLM (baseline)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cf73e526e94f58b131edebe5120cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932c52a10b0747d0b5a06e86c5194680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8587a5139b0940bc94158fad025c85ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd79e491a1464c1a80fdf033f0ba5817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ef37c3e02041d39a3f6b0c5e66922c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b51e49b20d447aa40ba9d18b52b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21c87f7632c49feb2f45fb7b0a0655b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b567204655d449f394e344e2fae9220c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74e080bc1f94ac1b7003bc6b41ef80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a473d83c7f4e98af8cdac53e2fe1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310606b6cdb944daa7c27a5d3e81818a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Embedding documents with MiniLM (baseline)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f0c2e1325e4c30ab0c4ae8a916a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building FAISS index for MiniLM (baseline)...\n",
      "\n",
      "🔍 Loading model: SciBERT (NLI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name gsarti/scibert-nli. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598fec2f85f64821a6526e9e3b16ed1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abdb2a21cae4c889ea12a3bfebf0a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5448f34b0f41ed9844df146be19500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/135 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9daf7b83ff7548d38b2d5fb72bea4fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f99eb9125f419390608b4f3bd142e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a237b317e0d449a1975f3cf3f04bb128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Embedding documents with SciBERT (NLI)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405421ce81d545479b2a1a7fefdd2bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Building FAISS index for SciBERT (NLI)...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Enter your medical query (or 'exit'):  pneumonia oxygen levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 3 results from BioBERT ===\n",
      "\n",
      "Result 1 (Score: 173.52):\n",
      "Admission Date:  [**2183-8-6**]       Discharge Date: [**2183-9-15**]   Service:  PRIMARY DIAGNOSIS: 1.  Volume overload anasarca. 2.  Status post posterior trach perforation and repair. 3.  Respiratory failure with ventilatory dependence.     Mechanical ventilatory dependence. 4.  Atrial fibrillati\n",
      "\n",
      "Result 2 (Score: 178.54):\n",
      "BackgroundNeonates are more susceptible to acute respiratory failure than older  children. It is unknown to what extent high-flow nasal cannula (HFNC) alters  intrathoracic pressure (ITP), potentially decreasing cardiac output (CO) due to  cardiopulmonary interactions. This study evaluated the impac\n",
      "\n",
      "Result 3 (Score: 185.37):\n",
      "Ascent to high altitude is accompanied by physiological responses that, to an  extent, mitigate the challenge of hypobaric hypoxia, maintaining arterial oxygen  content and convective oxygen delivery. Nevertheless, arterial oxygen tension  (pO2) remains low and tissue hypoxia persists, posing a chal\n",
      "\n",
      "=== Top 3 results from SapBERT ===\n",
      "\n",
      "Result 1 (Score: 185.77):\n",
      "BackgroundNeonates are more susceptible to acute respiratory failure than older  children. It is unknown to what extent high-flow nasal cannula (HFNC) alters  intrathoracic pressure (ITP), potentially decreasing cardiac output (CO) due to  cardiopulmonary interactions. This study evaluated the impac\n",
      "\n",
      "Result 2 (Score: 196.51):\n",
      "The primary cause of death from opioid overdose is opioid-induced respiratory  depression (OIRD), characterized by severe suppression of respiratory rate,  destabilized breathing patterns, hypercapnia, and heightened risk of apnea. The  retrotrapezoid nucleus (RTN), a critical chemosensitive brainst\n",
      "\n",
      "Result 3 (Score: 199.84):\n",
      "BACKGROUND AND OBJECTIVE: Breathlessness is associated with higher rates of  unplanned health service utilisation. We aimed to evaluate any associations  between the severity of breathlessness limiting exertion (hereafter  breathlessness), time between breathlessness recording and subsequent unplann\n",
      "\n",
      "=== Top 3 results from MiniLM (baseline) ===\n",
      "\n",
      "Result 1 (Score: 0.94):\n",
      "Admission Date:  [**2155-1-2**]              Discharge Date:   [**2155-1-6**]  Date of Birth:  [**2074-12-4**]             Sex:   M  Service: MEDICINE  Allergies: Patient recorded as having No Known Allergies to Drugs  Attending:[**First Name3 (LF) 898**] Chief Complaint: Dyspnea on exertion  Major \n",
      "\n",
      "Result 2 (Score: 0.98):\n",
      "Admission Date:  [**2149-9-2**]       Discharge Date:  Date of Birth:   [**2081-3-23**]       Sex:  M  Service:  CHIEF COMPLAINT:  Shortness of breath, cough, increased dyspnea on exertion.  HISTORY OF PRESENT ILLNESS:  This is a 68 year old male with a history of congestive heart failure and an eje\n",
      "\n",
      "Result 3 (Score: 0.98):\n",
      "Admission Date:  [**2178-12-5**]              Discharge Date:   [**2178-12-21**]  Date of Birth:  [**2114-2-8**]             Sex:   M  Service: MEDICINE  Allergies: Doxepin / Levofloxacin / Oxycontin  Attending:[**First Name3 (LF) 287**] Chief Complaint: sepsis  Major Surgical or Invasive Procedure:\n",
      "\n",
      "=== Top 3 results from SciBERT (NLI) ===\n",
      "\n",
      "Result 1 (Score: 412.92):\n",
      "Conflict of interest statement: Declarations. Conflict of interest:  P.E. Krumpoeck, G.\n",
      "\n",
      "Result 2 (Score: 422.09):\n",
      "DOI: 10.1097/01.NEP.\n",
      "\n",
      "Result 3 (Score: 422.09):\n",
      "DOI: 10.1097/01.NEP.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Enter your medical query (or 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# -------------------\n",
    "# CONFIG\n",
    "# -------------------\n",
    "MIMIC_FILE = \"final_combined_notes.csv\"\n",
    "PUBMED_FILE = \"pubmed_abstracts.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "# Models to compare\n",
    "MODELS = {\n",
    "    \"BioBERT\": \"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\",\n",
    "    \"SapBERT\": \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\",\n",
    "    \"MiniLM (baseline)\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"SciBERT (NLI)\": \"gsarti/scibert-nli\"\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# LOAD DATA\n",
    "# -------------------\n",
    "def load_mimic_notes(csv_path, text_column=\"TEXT\"):\n",
    "    df = pd.read_csv(csv_path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    df = df.dropna(subset=[text_column])\n",
    "    return df[text_column].tolist()\n",
    "\n",
    "def load_pubmed_abstracts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    return lines\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# BUILD EMBEDDINGS + INDEX\n",
    "# -------------------\n",
    "def build_faiss_index(embeddings):\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "def embed_documents(model, documents):\n",
    "    return model.encode(documents, show_progress_bar=True, convert_to_numpy=True, batch_size=16)\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# MAIN PIPELINE\n",
    "# -------------------\n",
    "def main():\n",
    "    print(\"📄 Loading documents...\")\n",
    "    mimic_docs = load_mimic_notes(MIMIC_FILE)\n",
    "    pubmed_docs = load_pubmed_abstracts(PUBMED_FILE)\n",
    "    combined_docs = mimic_docs + pubmed_docs\n",
    "    print(f\"Total documents: {len(combined_docs)}\")\n",
    "\n",
    "    indexes = {}\n",
    "\n",
    "    for name, model_name in MODELS.items():\n",
    "        print(f\"\\n🔍 Loading model: {name}\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "\n",
    "        print(f\"🔬 Embedding documents with {name}...\")\n",
    "        embeddings = embed_documents(model, combined_docs)\n",
    "\n",
    "        print(f\"📦 Building FAISS index for {name}...\")\n",
    "        index = build_faiss_index(embeddings)\n",
    "        indexes[name] = (model, index, combined_docs)\n",
    "\n",
    "    # Query loop\n",
    "    while True:\n",
    "        query = input(\"\\n🧠 Enter your medical query (or 'exit'): \").strip()\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        for name, (model, index, documents) in indexes.items():\n",
    "            print(f\"\\n=== Top {TOP_K} results from {name} ===\")\n",
    "            query_vec = model.encode([query], convert_to_numpy=True)\n",
    "            D, I = index.search(query_vec, TOP_K)\n",
    "\n",
    "            for rank, idx in enumerate(I[0]):\n",
    "                snippet = documents[idx][:300].replace(\"\\n\", \" \")\n",
    "                print(f\"\\nResult {rank+1} (Score: {D[0][rank]:.2f}):\")\n",
    "                print(snippet)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b08975b-9ac5-40db-9b38-3225fe684ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2df3e0-694a-4e5e-9c9e-93da5c06ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "def main():\n",
    "    print(\"📄 Loading documents...\")\n",
    "    mimic_docs = load_mimic_notes(MIMIC_FILE)\n",
    "    pubmed_docs = load_pubmed_abstracts(PUBMED_FILE)\n",
    "    combined_docs = mimic_docs + pubmed_docs\n",
    "    print(f\"Total documents: {len(combined_docs)}\")\n",
    "\n",
    "    # Load models and create indexes\n",
    "    indexes = {}\n",
    "    for name, model_name in MODELS.items():\n",
    "        print(f\"\\n🔍 Loading model: {name}\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        embeddings = embed_documents(model, combined_docs)\n",
    "        index = build_faiss_index(embeddings)\n",
    "        indexes[name] = (model, index, combined_docs)\n",
    "\n",
    "    # Store scores for numeric comparison\n",
    "    results_summary = defaultdict(list)\n",
    "\n",
    "    # Query loop\n",
    "    while True:\n",
    "        query = input(\"\\n🧠 Enter your medical query (or 'exit'): \").strip()\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        for name, (model, index, documents) in indexes.items():\n",
    "            print(f\"\\n=== Top {TOP_K} results from {name} ===\")\n",
    "            query_vec = model.encode([query], convert_to_numpy=True)\n",
    "            D, I = index.search(query_vec, TOP_K)\n",
    "\n",
    "            # Store numeric metrics\n",
    "            results_summary[name].append({\n",
    "                \"top1\": D[0][0],\n",
    "                \"avg_topk\": np.mean(D[0]),\n",
    "                \"min_topk\": np.min(D[0]),\n",
    "                \"max_topk\": np.max(D[0])\n",
    "            })\n",
    "\n",
    "            for rank, idx in enumerate(I[0]):\n",
    "                snippet = documents[idx][:300].replace(\"\\n\", \" \")\n",
    "                print(f\"\\nResult {rank+1} (Score: {D[0][rank]:.2f}):\")\n",
    "                print(snippet)\n",
    "\n",
    "    # Summary after all queries\n",
    "    print(\"\\n📊 Summary of Model Performance Across Queries\")\n",
    "    summary_data = []\n",
    "    for name, scores in results_summary.items():\n",
    "        top1s = [s[\"top1\"] for s in scores]\n",
    "        avg_topks = [s[\"avg_topk\"] for s in scores]\n",
    "        summary_data.append({\n",
    "            \"Model\": name,\n",
    "            \"Avg Top-1 Distance\": np.mean(top1s),\n",
    "            \"Avg Top-K Distance\": np.mean(avg_topks),\n",
    "            \"Queries Run\": len(scores)\n",
    "        })\n",
    "    df_summary = pd.DataFrame(summary_data).sort_values(by=\"Avg Top-K Distance\")\n",
    "    print(df_summary.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc10233a-500e-463e-9413-e7acec5dbe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loading documents...\n",
      "Total documents: 4512\n",
      "\n",
      "🔍 Loading model: BioBERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370ae7fc27854deda745d8154d563bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading model: SapBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name cambridgeltl/SapBERT-from-PubMedBERT-fulltext. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e24d20adea34b18b54db98c08b60b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading model: MiniLM (baseline)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcaeb2447b884874a6ebc80f58c3d000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading model: SciBERT (NLI)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name gsarti/scibert-nli. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe7f94371504effaf222fda9c050db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Enter your medical query (or 'exit'):  pneumonia oxygen levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top 3 results from BioBERT ===\n",
      "\n",
      "Result 1 (Score: 173.52):\n",
      "Admission Date:  [**2183-8-6**]       Discharge Date: [**2183-9-15**]   Service:  PRIMARY DIAGNOSIS: 1.  Volume overload anasarca. 2.  Status post posterior trach perforation and repair. 3.  Respiratory failure with ventilatory dependence.     Mechanical ventilatory dependence. 4.  Atrial fibrillati\n",
      "\n",
      "Result 2 (Score: 178.54):\n",
      "BackgroundNeonates are more susceptible to acute respiratory failure than older  children. It is unknown to what extent high-flow nasal cannula (HFNC) alters  intrathoracic pressure (ITP), potentially decreasing cardiac output (CO) due to  cardiopulmonary interactions. This study evaluated the impac\n",
      "\n",
      "Result 3 (Score: 185.37):\n",
      "Ascent to high altitude is accompanied by physiological responses that, to an  extent, mitigate the challenge of hypobaric hypoxia, maintaining arterial oxygen  content and convective oxygen delivery. Nevertheless, arterial oxygen tension  (pO2) remains low and tissue hypoxia persists, posing a chal\n",
      "\n",
      "=== Top 3 results from SapBERT ===\n",
      "\n",
      "Result 1 (Score: 166.67):\n",
      "Admission Date:  [**2176-3-14**]     Discharge Date:  Date of Birth:                  Sex:  F  Service:  ADDENDUM TO HOSPITAL COURSE:  (Date of this addendum is [**2176-4-18**])  1.  PULMONARY:  The patient continued to do well on low levels of pressure support and was extubated on [**4-4**].  Over \n",
      "\n",
      "Result 2 (Score: 185.77):\n",
      "BackgroundNeonates are more susceptible to acute respiratory failure than older  children. It is unknown to what extent high-flow nasal cannula (HFNC) alters  intrathoracic pressure (ITP), potentially decreasing cardiac output (CO) due to  cardiopulmonary interactions. This study evaluated the impac\n",
      "\n",
      "Result 3 (Score: 196.51):\n",
      "The primary cause of death from opioid overdose is opioid-induced respiratory  depression (OIRD), characterized by severe suppression of respiratory rate,  destabilized breathing patterns, hypercapnia, and heightened risk of apnea. The  retrotrapezoid nucleus (RTN), a critical chemosensitive brainst\n",
      "\n",
      "=== Top 3 results from MiniLM (baseline) ===\n",
      "\n",
      "Result 1 (Score: 0.93):\n",
      "Admission Date:  [**2123-7-2**]       Discharge Date:  [**2123-7-19**]  Date of Birth:   [**2078-7-13**]       Sex:  F  Service:  INTERNAL MEDICINE  CHIEF COMPLAINT:  Shortness of breath.  HISTORY OF PRESENT ILLNESS:  The patient is a 44 year-old woman who was brought to the Emergency Department by \n",
      "\n",
      "Result 2 (Score: 0.94):\n",
      "Admission Date:  [**2155-1-2**]              Discharge Date:   [**2155-1-6**]  Date of Birth:  [**2074-12-4**]             Sex:   M  Service: MEDICINE  Allergies: Patient recorded as having No Known Allergies to Drugs  Attending:[**First Name3 (LF) 898**] Chief Complaint: Dyspnea on exertion  Major \n",
      "\n",
      "Result 3 (Score: 0.95):\n",
      "Admission Date:  [**2178-1-29**]              Discharge Date:   [**2178-2-2**]   Service: MEDICINE  Allergies: Patient recorded as having No Known Allergies to Drugs  Attending:[**First Name3 (LF) 4052**] Chief Complaint: shortness of breath  Major Surgical or Invasive Procedure: NA  History of Pres\n",
      "\n",
      "=== Top 3 results from SciBERT (NLI) ===\n",
      "\n",
      "Result 1 (Score: 412.92):\n",
      "Conflict of interest statement: Declarations. Conflict of interest:  P.E. Krumpoeck, G.\n",
      "\n",
      "Result 2 (Score: 422.09):\n",
      "DOI: 10.1097/01.NEP.\n",
      "\n",
      "Result 3 (Score: 422.09):\n",
      "DOI: 10.1097/01.NEP.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Enter your medical query (or 'exit'):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Summary of Model Performance Across Queries\n",
      "| Model             |   Avg Top-1 Distance |   Avg Top-K Distance |   Queries Run |\n",
      "|:------------------|---------------------:|---------------------:|--------------:|\n",
      "| MiniLM (baseline) |             0.931602 |             0.940506 |             1 |\n",
      "| BioBERT           |           173.519    |           179.144    |             1 |\n",
      "| SapBERT           |           166.674    |           182.986    |             1 |\n",
      "| SciBERT (NLI)     |           412.917    |           419.034    |             1 |\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd93405-5e67-4916-a617-85416a813d60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a4543-5cde-43d4-9e06-a68e7d3e6cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93b39c-380d-4bec-9711-81cd050fe4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

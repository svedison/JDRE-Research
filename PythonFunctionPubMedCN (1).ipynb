{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491e82ac-728d-4e79-b096-66946806a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.8.0.dev20250319+cu128)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mmm\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m389.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading regex-2025.7.34-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 kB\u001b[0m \u001b[31m293.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m305.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, scipy, safetensors, regex, joblib, hf-xet, faiss-cpu, scikit-learn, pandas, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.12.0 hf-xet-1.1.9 huggingface-hub-0.34.4 joblib-1.5.2 pandas-2.3.2 pytz-2025.2 regex-2025.7.34 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 threadpoolctl-3.6.0 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.0 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu sentence-transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df2064a9-2279-4271-b8dc-870e3f51407b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: biopython in /usr/local/lib/python3.11/dist-packages (1.85)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from biopython) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71bc5289-3ff6-4037-8f71-e7999f4d20cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.1.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
      "Downloading multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (348 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3b54f3-698a-4e16-8ed4-b6303e0714a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from time import sleep\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import random\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa05d233-de6c-477d-ab38-4ea30f266700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching PubMed for: biomedical research\n",
      "Found 9999 articles.\n",
      "Fetching batch: 1–100\n",
      "Fetching batch: 101–200\n",
      "Fetching batch: 201–300\n",
      "Fetching batch: 301–400\n",
      "Fetching batch: 401–500\n",
      "Fetching batch: 501–600\n",
      "Fetching batch: 601–700\n",
      "Fetching batch: 701–800\n",
      "Fetching batch: 801–900\n",
      "Fetching batch: 901–1000\n",
      "Fetching batch: 1001–1100\n",
      "Fetching batch: 1101–1200\n",
      "Fetching batch: 1201–1300\n",
      "Fetching batch: 1301–1400\n",
      "Fetching batch: 1401–1500\n",
      "Fetching batch: 1501–1600\n",
      "Fetching batch: 1601–1700\n",
      "Fetching batch: 1701–1800\n",
      "Fetching batch: 1801–1900\n",
      "Fetching batch: 1901–2000\n",
      "Fetching batch: 2001–2100\n",
      "Fetching batch: 2101–2200\n",
      "Fetching batch: 2201–2300\n",
      "Fetching batch: 2301–2400\n",
      "Fetching batch: 2401–2500\n",
      "Fetching batch: 2501–2600\n",
      "Fetching batch: 2601–2700\n",
      "Fetching batch: 2701–2800\n",
      "Fetching batch: 2801–2900\n",
      "Fetching batch: 2901–3000\n",
      "Fetching batch: 3001–3100\n",
      "Fetching batch: 3101–3200\n",
      "Fetching batch: 3201–3300\n",
      "Fetching batch: 3301–3400\n",
      "Fetching batch: 3401–3500\n",
      "Fetching batch: 3501–3600\n",
      "Fetching batch: 3601–3700\n",
      "Fetching batch: 3701–3800\n",
      "Fetching batch: 3801–3900\n",
      "Fetching batch: 3901–4000\n",
      "Fetching batch: 4001–4100\n",
      "Fetching batch: 4101–4200\n",
      "Fetching batch: 4201–4300\n",
      "Fetching batch: 4301–4400\n",
      "Fetching batch: 4401–4500\n",
      "Fetching batch: 4501–4600\n",
      "Fetching batch: 4601–4700\n",
      "Fetching batch: 4701–4800\n",
      "Fetching batch: 4801–4900\n",
      "Fetching batch: 4901–5000\n",
      "Fetching batch: 5001–5100\n",
      "Fetching batch: 5101–5200\n",
      "Fetching batch: 5201–5300\n",
      "Fetching batch: 5301–5400\n",
      "Fetching batch: 5401–5500\n",
      "Fetching batch: 5501–5600\n",
      "Fetching batch: 5601–5700\n",
      "Fetching batch: 5701–5800\n",
      "Fetching batch: 5801–5900\n",
      "Fetching batch: 5901–6000\n",
      "Fetching batch: 6001–6100\n",
      "Fetching batch: 6101–6200\n",
      "Fetching batch: 6201–6300\n",
      "Fetching batch: 6301–6400\n",
      "Fetching batch: 6401–6500\n",
      "Fetching batch: 6501–6600\n",
      "Fetching batch: 6601–6700\n",
      "Fetching batch: 6701–6800\n",
      "Fetching batch: 6801–6900\n",
      "Fetching batch: 6901–7000\n",
      "Fetching batch: 7001–7100\n",
      "Fetching batch: 7101–7200\n",
      "Fetching batch: 7201–7300\n",
      "Fetching batch: 7301–7400\n",
      "Fetching batch: 7401–7500\n",
      "Fetching batch: 7501–7600\n",
      "Fetching batch: 7601–7700\n",
      "Fetching batch: 7701–7800\n",
      "Fetching batch: 7801–7900\n",
      "Fetching batch: 7901–8000\n",
      "Fetching batch: 8001–8100\n",
      "Fetching batch: 8101–8200\n",
      "Fetching batch: 8201–8300\n",
      "Fetching batch: 8301–8400\n",
      "Fetching batch: 8401–8500\n",
      "Fetching batch: 8501–8600\n",
      "Fetching batch: 8601–8700\n",
      "Fetching batch: 8701–8800\n",
      "Fetching batch: 8801–8900\n",
      "Fetching batch: 8901–9000\n",
      "Fetching batch: 9001–9100\n",
      "Fetching batch: 9101–9200\n",
      "Fetching batch: 9201–9300\n",
      "Fetching batch: 9301–9400\n",
      "Fetching batch: 9401–9500\n",
      "Fetching batch: 9501–9600\n",
      "Fetching batch: 9601–9700\n",
      "Fetching batch: 9701–9800\n",
      "Fetching batch: 9801–9900\n",
      "Fetching batch: 9901–9999\n",
      "\n",
      "Done. Saved clean abstracts to: pubmed_abstracts.txt\n"
     ]
    }
   ],
   "source": [
    "Entrez.email = \"edisonzjy@gmail.com\"\n",
    "search_term = \"biomedical research\"\n",
    "max_articles = 10000\n",
    "batch_size = 100\n",
    "\n",
    "output_file = \"pubmed_abstracts.txt\"\n",
    "\n",
    "def fetch_pubmed_abstracts():\n",
    "    print(f\"Searching PubMed for: {search_term}\")\n",
    "    search_handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=max_articles)\n",
    "    record = Entrez.read(search_handle)\n",
    "    search_handle.close()\n",
    "\n",
    "    pmids = record[\"IdList\"]\n",
    "    print(f\"Found {len(pmids)} articles.\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for start in range(0, len(pmids), batch_size):\n",
    "            end = min(start + batch_size, len(pmids))\n",
    "            batch_pmids = pmids[start:end]\n",
    "            print(f\"Fetching batch: {start + 1}–{end}\")\n",
    "            try:\n",
    "                handle = Entrez.efetch(\n",
    "                    db=\"pubmed\",\n",
    "                    id=\",\".join(batch_pmids),\n",
    "                    rettype=\"abstract\",\n",
    "                    retmode=\"text\"\n",
    "                )\n",
    "                data = handle.read()\n",
    "                handle.close()\n",
    "\n",
    "                articles = []\n",
    "                raw_entries = data.split(\"\\n\\n\")\n",
    "                for entry in raw_entries:\n",
    "                    match = re.search(r\"(?:\\n\\n)?(?:[A-Z].+?\\.){2,}\", entry, re.DOTALL)\n",
    "                    if match:\n",
    "                        cleaned = match.group().strip().replace(\"\\n\", \" \")\n",
    "                        articles.append(cleaned)\n",
    "                for article in articles:\n",
    "                    f.write(article + \"\\n\")\n",
    "\n",
    "                sleep(0.5)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching batch {start + 1}–{end}: {e}\")\n",
    "                sleep(2)\n",
    "\n",
    "    print(f\"\\nDone. Saved clean abstracts to: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_pubmed_abstracts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e0accc-4435-4938-93a2-e51dde018a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_pipeline_clinicalbert.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --------------------- CONFIG ---------------------\n",
    "    \n",
    "MIMIC_FILE = \"final_combined_notes.csv\"\n",
    "PUBMED_FILE = \"pubmed_abstracts.txt\"\n",
    "TOP_K = 3\n",
    "\n",
    "# ------------------ LOAD MODEL --------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb\")\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    return model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16e9e84d-c319-4062-a7b8-61ff41a06abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ LOADERS -----------------------\n",
    "def load_mimic_notes_with_metadata(csv_path, text_column=\"TEXT\"):\n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        on_bad_lines='skip',   # Skip malformed rows\n",
    "        engine='python',       # Use Python engine (slower, more tolerant)\n",
    "    )\n",
    "    df = df.dropna(subset=[text_column])\n",
    "    documents = df[text_column].tolist()\n",
    "    metadata = df.drop(columns=[text_column]).to_dict(orient='records')\n",
    "    return documents, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "942b09a1-a762-4e74-8bcc-d135539c6fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed_documents(file_path):\n",
    "    documents = []\n",
    "    metadata = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        text = line.strip()\n",
    "        if text:\n",
    "            documents.append(text)\n",
    "            metadata.append({\"abstract_id\": idx + 1})\n",
    "\n",
    "    return documents, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad69080-9d08-466c-92eb-5fe4b3162eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ INDEXING ----------------------\n",
    "\n",
    "def build_faiss_index(documents, batch_size=16):\n",
    "    print(f\"Embedding {len(documents)} documents...\")\n",
    "    embeddings = get_bert_embeddings(documents, batch_size=batch_size)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "\n",
    "    return index, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00437c-4f65-4ee8-9318-847ba128c2b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bc70d36-f3fe-45ba-b5f8-093f355a80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ SEARCH ------------------------\n",
    "\n",
    "def search_documents(query, index, documents, metadata, top_k=5):\n",
    "    query_vec = get_bert_embeddings([query])\n",
    "    distances, indices = index.search(np.array(query_vec), top_k)\n",
    "\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        results.append({\n",
    "            \"document\": documents[idx][:500] + \"...\" if len(documents[idx]) > 500 else documents[idx],\n",
    "            \"metadata\": metadata[idx]\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5724c0ae-5d09-4c15-8af5-0a0254afde44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading MIMIC-III Notes ---\n",
      "Embedding 2068 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61ed12f0a30461195a9e2d8561f5a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading PubMed Abstracts ---\n",
      "Embedding 2429 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb13cab16194f14a3a064e35c4a3045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a medical query:  patient with pneumonia and low oxygen levels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top MIMIC-III Notes ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a75217698f684e8da11cecf9afb1394a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "SUBJECT_ID: 20143\n",
      "HADM_ID: 176761\n",
      "Drugs Prescribed: nan\n",
      "Procedures: nan\n",
      "Note Snippet: Admission Date:  [**2148-10-29**]     Discharge Date:  [**2148-10-29**]\n",
      "\n",
      "\n",
      "Service:\n",
      "\n",
      "CHIEF COMPLAINT:  Shortness of breath.\n",
      "\n",
      "HISTORY OF PRESENT ILLNESS:  The patient is an 89 year-old\n",
      "female with a history of congestive heart failure and\n",
      "osteomyelitis of her left femur who presented with shortness\n",
      "of breath and decreased O2 saturations on room air in the\n",
      "setting of two days of foul smelling diarrhea.  The patient\n",
      "lives at [**Hospital3 2558**] and was transferred today to [**Hospital1 1444**] Emer...\n",
      "\n",
      "Result 2\n",
      "SUBJECT_ID: 11866\n",
      "HADM_ID: 148480\n",
      "Drugs Prescribed: nan\n",
      "Procedures: nan\n",
      "Note Snippet: Admission Date:  [**2163-3-7**]       Discharge Date:\n",
      "\n",
      "Date of Birth:   [**2108-5-28**]       Sex:  M\n",
      "\n",
      "Service:  Medicine - [**Doctor Last Name **]\n",
      "\n",
      "CHIEF COMPLAINT:  Hypoxic respiratory failure and hemodynamic\n",
      "instability.\n",
      "\n",
      "HISTORY OF PRESENT ILLNESS:  The patient is a 54 year old\n",
      "white male with a history of deep vein thrombosis and\n",
      "pulmonary embolus, hypertension and atrial fibrillation who\n",
      "presented to the Emergency Department complaining of dyspnea\n",
      "and weakness.  He had recently been hospit...\n",
      "\n",
      "Result 3\n",
      "SUBJECT_ID: 20649\n",
      "HADM_ID: 177831\n",
      "Drugs Prescribed: nan\n",
      "Procedures: nan\n",
      "Note Snippet: Admission Date:  [**2154-12-14**]     Death Date:  [**2154-12-15**]\n",
      "\n",
      "\n",
      "Service:  MEDICINE/[**Doctor Last Name 1181**]\n",
      "HISTORY OF PRESENT ILLNESS:  The patient is a 78-year-old\n",
      "male with a history of encephalitis, oral cancer, presenting\n",
      "to Intensive Care Unit with shortness of breath and hypoxia\n",
      "secondary to a large pleural effusion.  While in the\n",
      "Intensive Care Unit, the patient had transient hypotension\n",
      "and had a large O2 requirement secondary to the large effusion\n",
      "and multiple pulmonary nodule...\n",
      "\n",
      "--- Top PubMed Abstracts ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6361a23c69d4d70885baeb5e3fdb3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "Abstract ID: 1132\n",
      "Abstract Snippet: BackgroundNeonates are more susceptible to acute respiratory failure than older  children. It is unknown to what extent high-flow nasal cannula (HFNC) alters  intrathoracic pressure (ITP), potentially decreasing cardiac output (CO) due to  cardiopulmonary interactions. This study evaluated the impact of flow titration  on tracheal pressure (a surrogate for ITP) and CO via HFNC in an established  porcine model of neonatal respiratory failure. Unlike prior research, this study  examines both ITP a...\n",
      "\n",
      "Result 2\n",
      "Abstract ID: 1929\n",
      "Abstract Snippet: The primary cause of death from opioid overdose is opioid-induced respiratory  depression (OIRD), characterized by severe suppression of respiratory rate,  destabilized breathing patterns, hypercapnia, and heightened risk of apnea. The  retrotrapezoid nucleus (RTN), a critical chemosensitive brainstem region in the  rostral ventrolateral medullary reticular formation, contains  Phox2b+/neuromedin-B (Nmb) propriobulbar neurons. These neurons, stimulated by  CO2/H+, regulate breathing to prevent r...\n",
      "\n",
      "Result 3\n",
      "Abstract ID: 1558\n",
      "Abstract Snippet: BACKGROUND: Hypophosphatasia (HPP) is a rare metabolic disorder caused by low  tissue-nonspecific alkaline phosphatase (ALP) activity, presenting symptoms from  bone demineralization to tooth loss. It affects multiple systems and is  diagnosed based on clinical symptoms, radiological findings, and lab tests. This  case report emphasizes considering HPP in patients with unexplained bone pain  and low ALP levels, especially with underlying osteopenia or osteoporosis. It  highlights the importance ...\n"
     ]
    }
   ],
   "source": [
    "# ------------------ MAIN --------------------------\n",
    "\n",
    "def main():\n",
    "    print(\"\\n--- Loading MIMIC-III Notes ---\")\n",
    "    mimic_docs, mimic_meta = load_mimic_notes_with_metadata(MIMIC_FILE)\n",
    "    mimic_index, _ = build_faiss_index(mimic_docs)\n",
    "\n",
    "    print(\"\\n--- Loading PubMed Abstracts ---\")\n",
    "    pubmed_docs, pubmed_meta = load_pubmed_documents(PUBMED_FILE)\n",
    "    pubmed_index, _ = build_faiss_index(pubmed_docs)\n",
    "\n",
    "    query = input(\"\\nEnter a medical query: \").strip()\n",
    "\n",
    "    print(\"\\n--- Top MIMIC-III Notes ---\")\n",
    "    mimic_results = search_documents(query, mimic_index, mimic_docs, mimic_meta, top_k=TOP_K)\n",
    "    for i, res in enumerate(mimic_results):\n",
    "        print(f\"\\nResult {i+1}\")\n",
    "        print(\"SUBJECT_ID:\", res[\"metadata\"].get(\"SUBJECT_ID\"))\n",
    "        print(\"HADM_ID:\", res[\"metadata\"].get(\"HADM_ID\"))\n",
    "        print(\"Drugs Prescribed:\", res[\"metadata\"].get(\"NUM_DRUGS\"))\n",
    "        print(\"Procedures:\", res[\"metadata\"].get(\"NUM_PROC_ITEMS\"))\n",
    "        print(\"Note Snippet:\", res[\"document\"])\n",
    "\n",
    "    print(\"\\n--- Top PubMed Abstracts ---\")\n",
    "    pubmed_results = search_documents(query, pubmed_index, pubmed_docs, pubmed_meta, top_k=TOP_K)\n",
    "    for i, res in enumerate(pubmed_results):\n",
    "        print(f\"\\nResult {i+1}\")\n",
    "        print(\"Abstract ID:\", res[\"metadata\"].get(\"abstract_id\"))\n",
    "        print(\"Abstract Snippet:\", res[\"document\"])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8c23fe-c62e-41e7-94c1-4e27e74a2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kebiolm\n",
    "# sapbert once\n",
    "# biolinkbert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
